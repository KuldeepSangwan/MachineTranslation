{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac061d3b-8602-4f07-9d2c-d0d58f2adf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "from tqdm import tqdm\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "783bdd55-d3a7-40b0-b43b-524804335c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontractions(phrase):\n",
    "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert all the text into lower letters\n",
    "    # use this function to remove the contractions: https://gist.github.com/anandborad/d410a49a493b56dace4f814ab5325bbd\n",
    "    # remove all the spacial characters: except space ' '\n",
    "    text = text.lower()\n",
    "    text = decontractions(text)\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\u200d', ' ', text)\n",
    "    text = re.sub('\\u200c', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub('  ', ' ', text)\n",
    "    text = re.sub('   ', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_ita(text):\n",
    "    # convert all the text into lower letters\n",
    "    # remove the words betweent brakets ()\n",
    "    # remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    # replace these spl characters with space: '\\u200b', '\\xa0', '-', '/'\n",
    "    # we have found these characters after observing the data points, feel free to explore more and see if you can do find more\n",
    "    # you are free to do more proprocessing\n",
    "    # note that the model will learn better with better preprocessed data \n",
    "    text = re.sub(r\"([])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub('[$)\\\"’°;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\u200d', ' ', text)\n",
    "    text = re.sub('\\u200c', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    text = re.sub('  ', ' ', text)\n",
    "    text = re.sub('   ', ' ', text)\n",
    "    text =\" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w', encoding=\"utf-8\") as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d241d19-342f-4d8d-b59f-7b6122d10642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabFile(path, saving_path):\n",
    "    with open(path,'r', encoding=\"utf-8\") as file:\n",
    "        data=[]\n",
    "        for i in tqdm(file.readlines()):\n",
    "            data.append(i)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    bert_tokenizer_params=dict(lower_case=True)\n",
    "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "    bert_vocab_args = dict(\n",
    "        # The target vocabulary size\n",
    "        vocab_size = 150000,\n",
    "        # Reserved tokens that must be included in the vocabulary\n",
    "        reserved_tokens=reserved_tokens,\n",
    "        # Arguments for `text.BertTokenizer`\n",
    "        bert_tokenizer_params=bert_tokenizer_params,\n",
    "        # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    )\n",
    "    english_vocab = bert_vocab.bert_vocab_from_dataset(dataset.batch(1000),**bert_vocab_args)\n",
    "    write_vocab_file(saving_path, english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f983f74-0893-487c-ba4e-7bfeacf1f358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 548000/548000 [00:00<00:00, 1927820.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 548000/548000 [00:00<00:00, 1546611.82it/s]\n"
     ]
    }
   ],
   "source": [
    "create_vocabFile('Data/English-malyalam/train.en', 'Data/English-malyalam/english_vocab_all.txt')\n",
    "#use preprocess_ita\n",
    "create_vocabFile('Data/English-malyalam/train.mly', 'Data/English-malyalam/malayalam_vocab_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e09fc98b-8acd-4701-abe7-621f1412f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(\n",
    "    dataset, 'Data/English-malyalam/english_dataset', compression=None, shard_func=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ad738e2-f9e2-412e-9b8f-bc6b88da40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = tf.data.experimental.load('Data/English-malyalam/english_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27646a0-de6b-4046-942d-42ec50c79c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "\n",
    "train_en = train_examples.map(lambda pt, en: en)\n",
    "train_pt = train_examples.map(lambda pt, en: pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4516e3a-a22c-4a2e-a9ac-eaeeda48f0f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MapDataset' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0fbee3f00955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'MapDataset' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "train_en.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ede387-ea11-40a6-9829-190f458f7a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
